#notes #books #understandingDS #coordination


# System Models
Before we coordinate between the participants in a distributed system, there is some system models along which the system behaves.

1. Communication Link
	1. Fair Loss Link -> messages may be lost and duplicated
	2. Reliable Link -> message are delivered exactly once without loss/duplication
	3. Authenticated Reliable Link -> authenticate the sender + reliable link
2. Types of Failures
	1. Arbitrary Fault -> arbitrary deviation, [Byzantine Model](https://lamport.azurewebsites.net/pubs/byz.pdf)
	2. Crash Recovery -> crash and recover any time
	3. Crash Stop -> crash and doesn't recover
3. Timing Assumptions
	1. Synchronous -> defined amount of time for a given operation
	2. Asynchronous -> unbounded amount of time for a given operation
	3. Partially Synchronous -> real world systems

The major assumptions about the real world models are that they are `fair-loss links`, `crash recovery` processes which are `partially synchronous`

Further Readings
[Unreliable Failure Detectors](https://www.cs.utexas.edu/~lorenzo/corsi/cs380d/papers/p225-chandra.pdf)-> Paper
[Reliable and Secure Distributed Programming](https://www.distributedprogramming.net/) -> Book
[All models are wrong](https://en.wikipedia.org/wiki/All_models_are_wrong) -> Article

# Failure Detection

through pings and heartbeats at its core

# Time

1. Physical Clocks -> need to be synchronized through the NTP
2. Logical Clocks -> Measure the passage of time in terms of the operations (Lamport Clocks)
3. Vector Clocks - > Implemented with an array of counters, for each process in the system. Ensure that the lesser timestamp guarantees one operation happened before another.

Further Reading

[Atomic Clocks](https://en.wikipedia.org/wiki/Atomic_clock) -> Article
[NTP](https://datatracker.ietf.org/doc/html/rfc5905) -> RFC
[Time, Clocks and Ordering](https://lamport.azurewebsites.net/pubs/time-clocks.pdf) -> Paper
[Logical Clocks are Easy](https://queue.acm.org/detail.cfm?id=2917756) -> Article
[Timestamps in Message Passing Systems](https://fileadmin.cs.lth.se/cs/Personal/Amr_Ergawy/dist-algos-papers/4.pdf) -> Paper

# Consensus and Contention

## Leader Election

when the single process needs to access the shared resource, there needs to be a governing authority which guarantees safety and liveness

### Raft Leader Election

![Pasted Image](../Images/Pasted%20image%2020250204004544.png)

Further Reading

[Raft](https://raft.github.io/raft.pdf) -> Paper
[Paxos Made Simple](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf) -> Paper
[Paxos Made Abstract](https://maheshba.bitbucket.io/blog/2021/11/15/Paxos.html) -> Article
[Paxos Made Live](https://static.googleusercontent.com/media/research.google.com/en//archive/paxos_made_live.pdf) -> Paper
# Replication

replication is done to increase the availability of the system, increases the scalability and the performance of the distributed system

## State Machine Replication

the system elects a leader which algos like Raft, leader stores a sequence of the operation in a local log, which it replicated to its followers.

This is coupled with the Append Entries, which ensure that the follower nodes only commit the new operation once a consensus of the followers have received / which ensures a majority of the nodes have the update image

Consensus

ensures that group of nodes decide on a value
• every non-faulty process eventually agrees on a value; 
• the final decision of every non-faulty process is the same everywhere;
• and the value that has been agreed on has been proposed by a process.

Further Readings
[Finite State Machine](https://en.wikipedia.org/wiki/Finite-state_machine) -> Article
[Consensus](<https://en.wikipedia.org/wiki/Consensus_(computer_science)>) -> Article
[ETCD](https://etcd.io/) / [Zookeeper](https://zookeeper.apache.org/)

## Consistency Models

now that we have learnt the basic process to replicate the data across the nodes, there are some consistency levels associated to it, basically whats the deviation between the nodes and how long they take to come in order

1. Strong Consistency -> client exclusively queries the leader, slow but sure process
2. [Sequential Consistency](https://jepsen.io/consistency/models/sequential) -> allows followers to handle the request, guarantees that the all the operations occur in the same order, but not the time it takes to occur on each
3. Eventual Consistency -> The known scenario where the system will eventually converge but there will be some time it can server different results for the same query because the follower nodes are deviated.

Further Reading
[Linearizibility](https://jepsen.io/consistency/models/linearizable) -> Article
[Cap Theorem](https://www.youtube.com/watch?v=hUd_9FENShA) -> YT
[CAP Theorem perspective](https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf) -> Paper
[Critique of the CAP Theorem](https://www.cl.cam.ac.uk/research/dtg/archived/files/publications/public/mk428/cap-critique.pdf) -> Paper

[PACELC Theorem](https://en.wikipedia.org/wiki/PACELC_theorem) -> Article

[Consistency Levels in Azure](https://learn.microsoft.com/en-us/azure/cosmos-db/consistency-levels) -> Articles
[Consistency Levels in Cassandra](https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlConfigConsistency.html) -> Articles


## Chain Replication

this is a replication protocol , this is more performant than the raft algo, because we are ensuring consistency / replication as well as not dependent on the leader node to conduct the process / or the election process

![Pasted Image](../Images/Pasted%20image%2020250204010652.png)
[Chain Replication](https://www.cs.cornell.edu/home/rvr/papers/OSDI04.pdf) -> Paper
[Object Storage on CRAQ](https://www.usenix.org/legacy/event/usenix09/tech/full_papers/terrace/terrace.pdf) -> Paper